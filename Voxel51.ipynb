{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pzz0D_RZ7yA"
      },
      "source": [
        "Goal: We want to be able to visualize what points each batch selection method is choosing at each part of the training process of a model.\n",
        "\n",
        "Label each point with the following - loss value, batch #, if included in selection method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI2LQi_VOMXB"
      },
      "source": [
        "# Set Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Iw3ldY_RMdF"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0_7gNm7DOK8D",
        "outputId": "de8b1ec0-be9b-477d-9e1a-97575bf1bbc7"
      },
      "outputs": [],
      "source": [
        "import fiftyone.zoo as foz\n",
        "import cv2\n",
        "import numpy as np\n",
        "import fiftyone.brain as fob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import models\n",
        "import yaml\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import fiftyone as fo # Import fiftyone\n",
        "from PIL import Image # Import Image\n",
        "import detectors\n",
        "import timm\n",
        "#import transformers\n",
        "\n",
        "\n",
        "# resnet18_cifar10 = timm.create_model(\"resnet18_cifar10\", pretrained=True)\n",
        "# resnet34_cifar10 = timm.create_model(\"resnet34_cifar10\", pretrained=True)\n",
        "# resnet50_cifar10 = timm.create_model(\"resnet50_cifar10\", pretrained=True)\n",
        "# resnet34_supcon_cifar10 = timm.create_model(\"resnet34_supcon_cifar10\", pretrained=True)\n",
        "resnet50_supcon_cifar10 = timm.create_model(\"resnet50_supcon_cifar10\", pretrained=True)\n",
        "#resnet50_simclr_cifar10 = timm.create_model(\"resnet50_simclr_cifar10\", pretrained=True)\n",
        "pretrained_models = {\n",
        "    # \"resnet18_cifar10\": resnet18_cifar10,\n",
        "    # \"resnet34_cifar10\": resnet34_cifar10,\n",
        "    # \"resnet50_cifar10\": resnet50_cifar10,\n",
        "    # \"resnet34_supcon_cifar10\": resnet34_supcon_cifar10,\n",
        "    \"resnet50_supcon_cifar10\": resnet50_supcon_cifar10,\n",
        "    #\"resnet50_simclr_cifar10\": resnet50_simclr_cifar10\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSdtZppARHTo"
      },
      "source": [
        "## Set up ResNet model checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8HnVuk78RFyJ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read config file\n",
        "config_file = \"cfg/cifar10.yaml\"\n",
        "with open(config_file, 'r') as f:\n",
        "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
        "        f.close()\n",
        "\n",
        "model_type = config['networks']['type']\n",
        "model_args = config['networks']['params']\n",
        "empty_model = getattr(models, model_type)(**model_args)\n",
        "\n",
        "checkpoints = ['visualization_checkpoints/checkpoint_epoch_' + str(checkpoint) + '.pth.tar' for checkpoint in range(0,200,25)]\n",
        "model_params = torch.load(checkpoints[0], map_location=torch.device('cpu'), weights_only=False)\n",
        "empty_model.load_state_dict(model_params['state_dict'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3rkwE7BkLSW"
      },
      "source": [
        "# FiftyOne Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "sXUxxHnSlXlK"
      },
      "outputs": [],
      "source": [
        "def prepare_embeddings(model, model_name, dataset):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    batch_size = 128\n",
        "    embeddings = []\n",
        "\n",
        "    # Resize, normalize, and convert images to (C, H, W)\n",
        "    target_size = (32, 32)  # required input size for the CNN\n",
        "\n",
        "    image_tensors = []\n",
        "    for f in dataset.values(\"filepath\"):\n",
        "        img = cv2.imread(f, cv2.IMREAD_COLOR)  # force 3-channel BGR\n",
        "        img = cv2.resize(img, target_size)\n",
        "        img = img.astype(np.float32) / 255.0  # normalize to [0,1]\n",
        "        img = img.transpose(2, 0, 1)  # convert from (H, W, C) to (C, H, W)\n",
        "        image_tensors.append(img)\n",
        "\n",
        "    for i in range(0, len(image_tensors), batch_size):\n",
        "        batch_np = np.stack(image_tensors[i:i+batch_size])\n",
        "        batch_tensor = torch.tensor(batch_np)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model(batch_tensor)\n",
        "            embeddings.append(out.cpu().numpy())\n",
        "\n",
        "    embedding_outputs = np.concatenate(embeddings, axis=0)\n",
        "\n",
        "    results = fob.compute_visualization(\n",
        "        dataset,\n",
        "        embeddings=embedding_outputs,\n",
        "        num_dims=2,\n",
        "        method=\"umap\",\n",
        "        brain_key=model_name+\"_test\",\n",
        "        verbose=True,\n",
        "        seed=51,\n",
        "    )\n",
        "    dataset.load_brain_results(model_name+\"_test\")\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "FSN-NPvG9G-g",
        "outputId": "c6a6b0b0-b525-4f3e-ebea-f69e240cf155"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split 'test' already downloaded\n",
            "Loading existing dataset 'cifar10-test'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n",
            "Generating visualization...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/phancock/Online-Batch-Selection/.venv/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UMAP(n_jobs=1, random_state=51, verbose=True)\n",
            "Wed Sep 24 13:20:41 2025 Construct fuzzy simplicial set\n",
            "Wed Sep 24 13:20:41 2025 Finding Nearest Neighbors\n",
            "Wed Sep 24 13:20:41 2025 Building RP forest with 10 trees\n",
            "Wed Sep 24 13:20:48 2025 NN descent for 13 iterations\n",
            "\t 1  /  13\n",
            "\t 2  /  13\n",
            "\t 3  /  13\n",
            "\t 4  /  13\n",
            "\tStopping threshold met -- exiting after 4 iterations\n",
            "Wed Sep 24 13:21:00 2025 Finished Nearest Neighbor Search\n",
            "Wed Sep 24 13:21:03 2025 Construct embedding\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs completed:   2%| ▏          8/500 [00:00]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tcompleted  0  /  500 epochs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs completed:  12%| █▏         58/500 [00:01]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tcompleted  50  /  500 epochs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs completed:  22%| ██▏        108/500 [00:03]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tcompleted  100  /  500 epochs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs completed:  32%| ███▏       158/500 [00:04]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tcompleted  150  /  500 epochs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs completed:  42%| ████▏      208/500 [00:05]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tcompleted  200  /  500 epochs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs completed:  52%| █████▏     258/500 [00:06]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tcompleted  250  /  500 epochs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs completed:  62%| ██████▏    308/500 [00:07]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tcompleted  300  /  500 epochs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs completed:  72%| ███████▏   358/500 [00:08]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tcompleted  350  /  500 epochs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs completed:  82%| ████████▏  408/500 [00:10]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tcompleted  400  /  500 epochs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs completed:  92%| █████████▏ 458/500 [00:11]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tcompleted  450  /  500 epochs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs completed: 100%| ██████████ 500/500 [00:12]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Sep 24 13:21:15 2025 Finished embedding\n"
          ]
        }
      ],
      "source": [
        "# Dataset shown in fiftyone application\n",
        "test_split = foz.load_zoo_dataset(\"cifar10\", split=\"test\")\n",
        "\n",
        "for model_name, model in pretrained_models.items():\n",
        "    prepare_embeddings(model, model_name, test_split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"800\"\n",
              "            src=\"http://localhost:5151/?notebook=True&subscription=eea5d7c2-b99a-4140-9a8c-a18af12b25ab\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x746915ce6230>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Notebook sessions cannot wait\n"
          ]
        }
      ],
      "source": [
        "session = fo.launch_app(test_split)\n",
        "session.wait()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMXjCVba1vg6"
      },
      "source": [
        "# Add tags to dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G41odTNh8iqP"
      },
      "source": [
        "## Set up consistent dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAP92qAN6GQU",
        "outputId": "f5d4557d-5891-42a9-d7b4-1105e3bea3c7"
      },
      "outputs": [],
      "source": [
        "# Define the FiftyOneCIFARDataset class (copied from cell_id: -zonHH4HkINl)\n",
        "class FiftyOneCIFARDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, fiftyone_dataset, class_to_idx, transform=None):\n",
        "        self.dataset = fiftyone_dataset\n",
        "        self.transform = transform\n",
        "        self.sample_ids = list(self.dataset.values(\"_id\"))  # List of sample IDs\n",
        "        self.class_to_idx = class_to_idx # Store the class_to_idx mapping\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sample_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample_id = self.sample_ids[idx]\n",
        "        sample = self.dataset[sample_id]  # Access by sample ID\n",
        "        img = Image.open(sample.filepath).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        # Convert string label to integer index using the mapping\n",
        "        label_str = sample.ground_truth.label\n",
        "        label_int = self.class_to_idx[label_str]\n",
        "        label_tensor = torch.tensor(label_int, dtype=torch.long) # Ensure label is a LongTensor\n",
        "\n",
        "        return img, label_tensor, str(sample_id)  # Return tensor label and sample_id as string\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Use train_dataset to get class_to_idx function\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "class_to_idx = train_dataset.class_to_idx\n",
        "\n",
        "# Instantiate the original Fiftyone dataset and custum FiftyOne dataset and dataloader\n",
        "test_split = foz.load_zoo_dataset(\"cifar10\", split=\"test\")\n",
        "custom_fiftyone_dataset = FiftyOneCIFARDataset(test_split, class_to_idx=class_to_idx, transform=transform)\n",
        "custom_fiftyone_dataloader = DataLoader(custom_fiftyone_dataset, batch_size=64, shuffle=False) # Use shuffle=False for consistent batching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tag class probabilities to each sample in the fiftyone dataset\n",
        "def tag_class_probabilities(fo_dataset, custom_dataset, class_index, epoch, probabilities):\n",
        "    if not fo_dataset.has_field(f\"class_{class_index}_prob\"):\n",
        "        fo_dataset.add_sample_field(f\"class_{class_index}_prob\", fo.FloatField)\n",
        "\n",
        "    sample_ids = custom_dataset.sample_ids\n",
        "\n",
        "    for i, sample_id in enumerate(sample_ids):\n",
        "        sample = fo_dataset[sample_id]\n",
        "        sample[f\"epoch_{epoch}_class_{class_index}_prob\"] = float(probabilities[i, class_index].item())\n",
        "        sample.save()  # <-- Save each modified sample\n",
        "\n",
        "for checkpoint in [checkpoints[0], checkpoints[-1]]:\n",
        "    model = empty_model\n",
        "    model.load_state_dict(torch.load(checkpoint, map_location=torch.device('cpu'), weights_only=False)['state_dict'])\n",
        "\n",
        "    target_size = (32, 32)  # required input size for your CNN\n",
        "    image_tensors = []\n",
        "    for f in test_split.values(\"filepath\"):\n",
        "        img = cv2.imread(f, cv2.IMREAD_COLOR)  # force 3-channel BGR\n",
        "        img = cv2.resize(img, target_size)\n",
        "        img = img.astype(np.float32) / 255.0  # normalize to [0,1]\n",
        "        img = img.transpose(2, 0, 1)  # convert from (H, W, C) to (C, H, W)\n",
        "        image_tensors.append(img)\n",
        "\n",
        "    output = model(torch.tensor(image_tensors))\n",
        "    probabilities_softmax = nn.Softmax(dim=1)(output)\n",
        "    epoch = checkpoint.split('_')[-1].split('.')[0]\n",
        "    # log_probabilities = nn.LogSoftmax(dim=1)(output)\n",
        "\n",
        "    for class_index in range(10):\n",
        "        tag_class_probabilities(test_split, custom_fiftyone_dataset, class_index, epoch, probabilities_softmax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C3K9h0R35Ck"
      },
      "source": [
        "## Tag batch number and loss values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TlVbVdQPOX5p"
      },
      "outputs": [],
      "source": [
        "# Might be wrong since the torch cifar10 dataset and fiftyone cifar10 dataset might differ in the ordering which would result in incorrect tagging\n",
        "\n",
        "def tag_losses(dataloader, dataset, checkpoints):\n",
        "    for checkpoint in checkpoints:\n",
        "        model = empty_model\n",
        "        print(f\"Loading model from path: {checkpoint}\") # Added print statement\n",
        "        # Load the model state dictionary, ensuring map_location is set if needed (e.g., if trained on GPU and loading on CPU)\n",
        "        try: # Added try-except block for loading model\n",
        "            model.load_state_dict(torch.load(checkpoint, map_location=torch.device('cpu'), weights_only=False)['state_dict'])\n",
        "            print(f\"Successfully loaded model from {checkpoint}\") # Added print statement\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model from {checkpoint}: {e}\")\n",
        "            continue # Skip to the next checkpoint if loading fails\n",
        "        \n",
        "        checkpoint_num = checkpoint.split('_')[-1].split('.')[0]\n",
        "        model_name = f'model_epoch_{checkpoint_num}'\n",
        "        criterion_none = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
        "        losses_by_id = {}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels, sample_ids in dataloader:\n",
        "                outputs = model(inputs)\n",
        "                individual_losses = criterion_none(outputs, labels)\n",
        "\n",
        "                for i, sample_id in enumerate(sample_ids):\n",
        "                    losses_by_id[sample_id] = round(individual_losses[i].item(), 4)\n",
        "\n",
        "        # Ensure the 'losses' field exists only once\n",
        "        if not dataset.has_field(\"losses\"):\n",
        "            dataset.add_sample_field(\"losses\", fo.DictField)\n",
        "\n",
        "        for sample_id, loss in losses_by_id.items():\n",
        "            sample = dataset[sample_id]\n",
        "            if sample.losses is None:\n",
        "                sample.losses = {}\n",
        "            sample.losses[model_name] = loss\n",
        "            sample[model_name + \"_loss\"] = loss\n",
        "            sample.save()\n",
        "\n",
        "\n",
        "def tag_batches(dataloader, fo_dataset, custom_dataset):\n",
        "    if not fo_dataset.has_field(\"batch_num\"):\n",
        "        fo_dataset.add_sample_field(\"batch_num\", fo.IntField)\n",
        "\n",
        "    sample_ids = custom_dataset.sample_ids\n",
        "    batch_size = dataloader.batch_size\n",
        "\n",
        "    for i, sample_id in enumerate(sample_ids):\n",
        "        batch_index = i // batch_size\n",
        "        sample = fo_dataset[sample_id]\n",
        "        sample[\"batch_num\"] = batch_index\n",
        "        sample.save()  # <-- Save each modified sample\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Criterion\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Call the tag_data function with the dataloader, dataset, criterion, and checkpoint paths\n",
        "tag_losses(custom_fiftyone_dataloader, test_split, checkpoints)\n",
        "tag_batches(custom_fiftyone_dataloader, test_split, custom_fiftyone_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXoIuIvSlKZ_"
      },
      "source": [
        "## Get Selected Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sIk2x7_lUS1"
      },
      "outputs": [],
      "source": [
        "from methods.SelectionMethod import SelectionMethod\n",
        "from methods.DivBS import DivBS\n",
        "from methods.Uniform import Uniform\n",
        "from methods.Bayesian import Bayesian\n",
        "import numpy as np\n",
        "import yaml\n",
        "from utils import custom_logger\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjT-RjlGL0ao"
      },
      "source": [
        "### Get selected points (DivBS & Uniform)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rNvwCPcfPNei",
        "outputId": "2a3b6145-92ce-4d6a-9309-544cb60ad3ce"
      },
      "outputs": [],
      "source": [
        "class Uniform_Selection(Uniform):\n",
        "    def get_indicies(self, dataset, epoch):\n",
        "\n",
        "        data_loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "        all_indexes = []\n",
        "\n",
        "        for i, datas in enumerate(data_loader):\n",
        "            inputs = np.array(datas[0])\n",
        "            targets = np.array(datas[1])\n",
        "            indexes = np.array(np.arange(inputs.shape[0]).tolist())\n",
        "\n",
        "            # Call batch selection logic\n",
        "            inputs, targets, indexes = self.before_batch(i, inputs, targets, indexes, epoch)\n",
        "            all_indexes.append(indexes)\n",
        "\n",
        "        return all_indexes\n",
        "\n",
        "\n",
        "class DivBS_Selection(DivBS):\n",
        "    def get_indicies(self, dataset, epoch):\n",
        "\n",
        "        data_loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "        all_indexes = []\n",
        "\n",
        "        for i, datas in enumerate(data_loader):\n",
        "            inputs = datas[0]\n",
        "            targets = datas[1]\n",
        "            indexes = np.arange(inputs.shape[0])\n",
        "\n",
        "            # Call batch selection logic\n",
        "            inputs, targets, indexes = self.before_batch(i, inputs, targets, indexes, epoch)\n",
        "            all_indexes.append(indexes)\n",
        "\n",
        "        return all_indexes\n",
        "\n",
        "class Bayesian_Selection(Bayesian):\n",
        "    def get_indicies(self, dataset, epoch):\n",
        "\n",
        "        data_loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "        all_indexes = []\n",
        "\n",
        "        for i, datas in enumerate(data_loader):\n",
        "            inputs = datas[0]\n",
        "            targets = datas[1]\n",
        "            indexes = np.arange(inputs.shape[0])\n",
        "\n",
        "            # Call batch selection logic\n",
        "            inputs, targets, indexes = self.before_batch(i, inputs, targets, indexes, epoch)\n",
        "            all_indexes.append(indexes)\n",
        "\n",
        "        return all_indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ND2KeCoLMXZH"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Set up logger\n",
        "try: # Added try-except block for setting up logger\n",
        "    logger = custom_logger(\"./exp\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    logger.info('device: ' + str(device))\n",
        "except Exception as e:\n",
        "    print(f\"Error setting up logger: {e}\")\n",
        "\n",
        "# Set up dataset\n",
        "test_split = foz.load_zoo_dataset(\"cifar10\", split=\"test\")\n",
        "custom_fiftyone_dataset = FiftyOneCIFARDataset(test_split, class_to_idx=class_to_idx, transform=transform)\n",
        "\n",
        "selected_indices_DivBS = []\n",
        "selected_indices_Uniform = []\n",
        "#selected_indices_Bayesian = []\n",
        "\n",
        "# NOTE: Can't start at checkpoint 0 because it raises an error with feature tensor becoming 0\n",
        "for checkpoint in checkpoints:\n",
        "\n",
        "    model = empty_model\n",
        "    print(f\"Loading model from path: {checkpoint}\") \n",
        "\n",
        "    try: # try-except block for loading model\n",
        "        model.load_state_dict(torch.load(checkpoint, map_location=torch.device('cpu'), weights_only=False)['state_dict'])\n",
        "        print(f\"Successfully loaded model from {checkpoint}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model from {checkpoint}: {e}\")\n",
        "        continue # Skip to the next checkpoint if loading fails\n",
        "    \n",
        "    checkpoint_num = int(checkpoint.split('_')[-1].split('.')[0])\n",
        "    \n",
        "    DivBS_selector = DivBS_Selection(config, logger)\n",
        "    Uniform_selector = Uniform_Selection(config, logger)\n",
        "    #Bayesian_selector = Bayesian_Selection(config, logger)\n",
        "    DivBS_selector.model = model\n",
        "    Uniform_selector.model = model\n",
        "    #Bayesian_selector.model = model\n",
        "    run_indicies_DivBS = DivBS_selector.get_indicies(custom_fiftyone_dataset, checkpoint_num)\n",
        "    run_indicies_Uniform = Uniform_selector.get_indicies(custom_fiftyone_dataset, checkpoint_num)\n",
        "    #run_indicies_Bayesian = Bayesian_selector.get_indicies(custom_fiftyone_dataset, checkpoint_num)\n",
        "    selected_indices_DivBS.append(run_indicies_DivBS)\n",
        "    selected_indices_Uniform.append(run_indicies_Uniform)\n",
        "    #selected_indices_Bayesian.append(run_indicies_Bayesian)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ4_ABNv4Jtk"
      },
      "source": [
        "## Tag selected points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TQ4BHjFoJlh1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import fiftyone as fo # Import fiftyone\n",
        "from PIL import Image # Import Image\n",
        "\n",
        "# Define the FiftyOneCIFARDataset class (copied from cell_id: -zonHH4HkINl)\n",
        "class FiftyOneCIFARDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, fiftyone_dataset, class_to_idx, transform=None):\n",
        "        self.dataset = fiftyone_dataset\n",
        "        self.transform = transform\n",
        "        self.sample_ids = list(self.dataset.values(\"_id\"))  # List of sample IDs\n",
        "        self.class_to_idx = class_to_idx # Store the class_to_idx mapping\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sample_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample_id = self.sample_ids[idx]\n",
        "        sample = self.dataset[sample_id]  # Access by sample ID\n",
        "        img = Image.open(sample.filepath).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        # Convert string label to integer index using the mapping\n",
        "        label_str = sample.ground_truth.label\n",
        "        label_int = self.class_to_idx[label_str]\n",
        "        label_tensor = torch.tensor(label_int, dtype=torch.long) # Ensure label is a LongTensor\n",
        "\n",
        "        return img, label_tensor, str(sample_id)  # Return tensor label and sample_id as string\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Use train_dataset to get class_to_idx function\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "class_to_idx = train_dataset.class_to_idx\n",
        "\n",
        "# Instantiate the original Fiftyone dataset and custum FiftyOne dataset and dataloader\n",
        "test_split = foz.load_zoo_dataset(\"cifar10\", split=\"test\")\n",
        "custom_fiftyone_dataset = FiftyOneCIFARDataset(test_split, class_to_idx=class_to_idx, transform=transform)\n",
        "custom_fiftyone_dataloader = DataLoader(custom_fiftyone_dataset, batch_size=64, shuffle=False) # Use shuffle=False for consistent batching\n",
        "\n",
        "\n",
        "def tag_selected_points(dataloader, fo_dataset, custom_dataset, checkpoints, selected_indices, method_name):\n",
        "    # Iterate over each checkpoint\n",
        "    for ckpt_idx, checkpoint in enumerate(checkpoints):\n",
        "        # Added try-except block for loading model\n",
        "        model = empty_model\n",
        "        print(f\"Loading model from path: {checkpoint}\") # Added print statement\n",
        "        # Load the model state dictionary\n",
        "        try: \n",
        "            model.load_state_dict(torch.load(checkpoint, map_location=torch.device('cpu'), weights_only=False)['state_dict'])\n",
        "            print(f\"Successfully loaded model from {checkpoint}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model from {checkpoint}: {e}\")\n",
        "            continue # Skip to the next checkpoint if loading fails\n",
        "        \n",
        "        checkpoint_num = checkpoint.split('_')[-1].split('.')[0]\n",
        "        model_name = f'model_epoch_{checkpoint_num}'\n",
        "\n",
        "        sample_ids = custom_dataset.sample_ids\n",
        "        batch_size = dataloader.batch_size\n",
        "\n",
        "        # # Added a check to ensure selected_indices has enough elements\n",
        "        # if ckpt_idx >= len(selected_indices):\n",
        "        #     print(f\"Warning: Not enough selected indices for checkpoint {model_name}. Skipping tagging.\")\n",
        "        #     continue\n",
        "\n",
        "        # Get selected indices for this checkpoint\n",
        "        batches_of_selected_points = selected_indices[ckpt_idx]\n",
        "\n",
        "        for i, sample_id in enumerate(sample_ids):\n",
        "            sample_batch_index = i // batch_size\n",
        "            in_batch_index = i % batch_size\n",
        "\n",
        "            # # Added a check to ensure selected_batches has enough elements\n",
        "            # if batch_index >= len(selected_batches):\n",
        "            #     print(f\"Warning: Not enough selected batches for sample {sample_id} in checkpoint {model_name}. Skipping tagging.\")\n",
        "            #     continue\n",
        "\n",
        "            # Get selected indices for this batch\n",
        "            batch_selected_points = batches_of_selected_points[sample_batch_index]\n",
        "\n",
        "            # # Added a check to ensure batch_selected_points is iterable\n",
        "            # if not hasattr(batch_selected_points, '__iter__'):\n",
        "            #      print(f\"Warning: batch_selected_points for sample {sample_id} in checkpoint {model_name} is not iterable. Skipping tagging.\")\n",
        "            #      continue\n",
        "\n",
        "            # if sample is in the selected points for this batch, tag it as True, else False\n",
        "            selected = in_batch_index in batch_selected_points\n",
        "            sample = fo_dataset[sample_id]\n",
        "            sample[model_name + \"_\" + method_name + \"_selected\"] = selected\n",
        "            sample.save()\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "tag_selected_points(custom_fiftyone_dataloader, test_split, custom_fiftyone_dataset, checkpoints, selected_indices_DivBS, \"DivBS\")\n",
        "tag_selected_points(custom_fiftyone_dataloader, test_split, custom_fiftyone_dataset, checkpoints, selected_indices_Uniform, \"Uniform\")\n",
        "# tag_selected_points(custom_fiftyone_dataloader, test_split, custom_fiftyone_dataset, checkpoints, selected_indices_Bayesian, \"Bayesian\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhDWWzBUb4qR"
      },
      "source": [
        "# Show Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12fXOKVuog65"
      },
      "outputs": [],
      "source": [
        "session = fo.launch_app(test_split)\n",
        "session.wait()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "v3rkwE7BkLSW",
        "2C3K9h0R35Ck"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "online-bs",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
